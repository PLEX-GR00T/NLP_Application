{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PLEX-GR00T/NLP/blob/main/CMPE_297_RE_TM_ED_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CMPE 297: Homework 1: Regular expressions, text normalization, and edit distance\n",
        "\n",
        "The parts that you need to complete are marked as Exercises."
      ],
      "metadata": {
        "id": "eGajpTvZbl8W"
      },
      "id": "eGajpTvZbl8W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Initialization & Setup"
      ],
      "metadata": {
        "id": "lkzAo9kSbpaa"
      },
      "id": "lkzAo9kSbpaa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d56c92a0",
      "metadata": {
        "id": "d56c92a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb79d40c-b908-4cbf-bdce-0877e4ac074e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# importing required libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Regular Expressions"
      ],
      "metadata": {
        "id": "2jEV5pB3bhkq"
      },
      "id": "2jEV5pB3bhkq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting license plate numbers, IDs, emails and mailing addresses from a document\n"
      ],
      "metadata": {
        "id": "oLP8HLkcbvcG"
      },
      "id": "oLP8HLkcbvcG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Document creation"
      ],
      "metadata": {
        "id": "xLT-yj6keZOF"
      },
      "id": "xLT-yj6keZOF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefa7f39",
      "metadata": {
        "id": "aefa7f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0002f58b-d721-47a8-f233-f3d11ab1caf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "sentence = 'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'\n",
        "sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting license plate numbers"
      ],
      "metadata": {
        "id": "enF7P05qebea"
      },
      "id": "enF7P05qebea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e2f32e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e2f32e",
        "outputId": "2f36c560-9435-43d1-a16d-6ae3e740f5c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4XUI302', '3A-278']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# The format of license plate number is a digit then 2 or 3 letters (one of which can be a \"-\"), and then 3 digits\n",
        "\n",
        "regex = re.compile(r'(\\d{1}[A-Za-z-]{2,3}\\d{3})')\n",
        "lincense_plate_numbers = regex.findall(sentence)\n",
        "lincense_plate_numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-1: Extract the ID numbers from the document."
      ],
      "metadata": {
        "id": "nzg5Gxx9dzW2"
      },
      "id": "nzg5Gxx9dzW2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d98e769f",
      "metadata": {
        "id": "d98e769f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5ab518-b903-4c75-954b-1d207f126b46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J987492']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# The format of the IDs is one character/letter and then 6 digits\n",
        "regex = re.compile(r'([A-Za-z]\\d{6})')\n",
        "ids = regex.findall(sentence)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-2: Extract the email IDs from the document"
      ],
      "metadata": {
        "id": "y3BZc47FeRzR"
      },
      "id": "y3BZc47FeRzR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11287af4",
      "metadata": {
        "id": "11287af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61588243-d18a-401c-f0ac-a0ee995805a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['myemail123+spam@google.cg', 'jane.doe@sjsu.edu']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "regex = re.compile(r'[a-zA-Z0-9-_.+]+@[a-zA-Z0-9-_.]+')\n",
        "emails = regex.findall(sentence)\n",
        "emails"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-3: Extract the mailing address from the document"
      ],
      "metadata": {
        "id": "UfCxo2u2erDf"
      },
      "id": "UfCxo2u2erDf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62515169",
      "metadata": {
        "id": "62515169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2294b60-a0e2-49fa-8a8c-250c740da00c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['123 Main street, San Jose, CA.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "regex = re.compile(r'\\d{1,5}\\s\\w*\\s\\w*,\\s\\w*\\s\\w*,\\s\\w*.')\n",
        "mailing_address = regex.findall(sentence)\n",
        "mailing_address"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-4: Anonymize the license plate numbers by replacing them with the text \"LP_NUM\"\n",
        "\n",
        "The re.sub function is described here: https://docs.python.org/3/library/re.html"
      ],
      "metadata": {
        "id": "uM53UdvPevrA"
      },
      "id": "uM53UdvPevrA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6ae08c",
      "metadata": {
        "id": "ca6ae08c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1b1a4661-d451-4b0d-c511-06ee872ee323"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am 20 years old. My previous license plate number was LP_NUM and my new one is LP_NUM. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Now replacing license plate numbers with the string \"LP_NUM\"\n",
        "sentence_modified = re.sub(r'\\d{1}[A-Za-z-]{2,3}\\d{3}',\n",
        "                           r'LP_NUM',\n",
        "                           sentence)\n",
        "sentence_modified"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1-5: Replace the ID numbers with the text \"ID_NUM\""
      ],
      "metadata": {
        "id": "rArCsPyMfAeZ"
      },
      "id": "rArCsPyMfAeZ"
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_modified_ID= re.sub(r'([A-Za-z]\\d{6})',\n",
        "                          r'ID_NUM',\n",
        "                          sentence)\n",
        "sentence_modified_ID"
      ],
      "metadata": {
        "id": "glVWmQAOfFTU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7fca0863-d870-44f7-e1cd-0c7a1382f1ad"
      },
      "id": "glVWmQAOfFTU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3A-278. My ID is ID_NUM and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.cg or jane.doe@sjsu.edu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Text Processing "
      ],
      "metadata": {
        "id": "q2ymEa7sfHnL"
      },
      "id": "q2ymEa7sfHnL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of words in the movie_reviews dataset (dataset uploaded in the beginning of this notebook under \"Part 0: Initialization and Setup\")"
      ],
      "metadata": {
        "id": "F_Y77JT9fjid"
      },
      "id": "F_Y77JT9fjid"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3239e17c",
      "metadata": {
        "id": "3239e17c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb90bae-a9e2-4bc4-8ea4-7aeb0a06b6b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1583820"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# print number of words in the movie review dataset\n",
        "len(movie_reviews.words())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the standard list of punctuation marks"
      ],
      "metadata": {
        "id": "86nbNEYwfgwl"
      },
      "id": "86nbNEYwfgwl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0929c795",
      "metadata": {
        "id": "0929c795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f59f72a4-cdb1-426a-f270-c3cfe1320f67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "punctuations = string.punctuation\n",
        "punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove punctation from movie reviews \n"
      ],
      "metadata": {
        "id": "sg5Sc7X9fbpg"
      },
      "id": "sg5Sc7X9fbpg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18760b2a",
      "metadata": {
        "id": "18760b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc6a5fc-dad1-403b-83b7-15883150c6ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1338788"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "words_wo_puncts = [x for x in movie_reviews.words() if x not in punctuations]\n",
        "len(words_wo_puncts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of unique words"
      ],
      "metadata": {
        "id": "Gvl4C9l8f0M-"
      },
      "id": "Gvl4C9l8f0M-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a18f452",
      "metadata": {
        "id": "0a18f452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d4be8b-fb89-423a-c113-47142cc0f0ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39737"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "unique_words = set(words_wo_puncts)\n",
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the 20 most frequent words in the dataset"
      ],
      "metadata": {
        "id": "D2L7yKz3gL-h"
      },
      "id": "D2L7yKz3gL-h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784d9a68",
      "metadata": {
        "id": "784d9a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712ea564-3946-45bc-e8f6-f291b432282f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "the     76529\n",
              "a       38106\n",
              "and     35576\n",
              "of      34123\n",
              "to      31937\n",
              "is      25195\n",
              "in      21822\n",
              "s       18513\n",
              "it      16107\n",
              "that    15924\n",
              "as      11378\n",
              "with    10792\n",
              "for      9961\n",
              "his      9587\n",
              "this     9578\n",
              "film     9517\n",
              "i        8889\n",
              "he       8864\n",
              "but      8634\n",
              "on       7385\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# top 20 highest freq words\n",
        "pd.Series(words_wo_puncts).value_counts()[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the standard list of stopwords"
      ],
      "metadata": {
        "id": "id9YqucXf6oW"
      },
      "id": "id9YqucXf6oW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de2e57e",
      "metadata": {
        "id": "9de2e57e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9f4039-582d-4e67-ba2c-93f38259b535"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# getting english stopwords\n",
        "eng_stopwords = stopwords.words('english')\n",
        "eng_stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of stopwords"
      ],
      "metadata": {
        "id": "cL-RrEeCgA0Z"
      },
      "id": "cL-RrEeCgA0Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6645fe0",
      "metadata": {
        "id": "d6645fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc7da57-8aba-4653-8497-192f7d35cf78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(eng_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-1: Remove the stopwords from the dataset (similarly to how we removed punctuation above)"
      ],
      "metadata": {
        "id": "xBAO2VeWgDiM"
      },
      "id": "xBAO2VeWgDiM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d584a6f",
      "metadata": {
        "id": "9d584a6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd0fba8-c5ac-4d11-cd5a-9b42c9598f0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "710578"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "words_wo_puncts_stopwords =[x for x in words_wo_puncts if x not in eng_stopwords] \n",
        "len(words_wo_puncts_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-2: Find the number of uniques words in the dataset now that the stop words have been removed"
      ],
      "metadata": {
        "id": "RuUty50kgS2o"
      },
      "id": "RuUty50kgS2o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92f5eb5",
      "metadata": {
        "id": "a92f5eb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fd20ce-87ee-49b8-f59b-de8d4cae9f99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39586"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# unique words without stopwords\n",
        "unique_words_wo_puncts_stopwords = set(words_wo_puncts_stopwords)\n",
        "len(unique_words_wo_puncts_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-3: Find the top 20 highest frequency words now that we have removed the stopwords"
      ],
      "metadata": {
        "id": "bxDxQWNegcny"
      },
      "id": "bxDxQWNegcny"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840dded3",
      "metadata": {
        "id": "840dded3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6775b49-41f1-4e8e-cbc2-55b6968ca000"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "film          9517\n",
              "one           5852\n",
              "movie         5771\n",
              "like          3690\n",
              "even          2565\n",
              "time          2411\n",
              "good          2411\n",
              "story         2169\n",
              "would         2109\n",
              "much          2049\n",
              "character     2020\n",
              "also          1967\n",
              "get           1949\n",
              "two           1911\n",
              "well          1906\n",
              "characters    1859\n",
              "first         1836\n",
              "--            1815\n",
              "see           1749\n",
              "way           1693\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# top 20 highest freq words after removing stopwords\n",
        "pd.Series(words_wo_puncts_stopwords).value_counts()[:20]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the words that are used only once in the corpus (and print the first few).  "
      ],
      "metadata": {
        "id": "9Z36G7BcgmDF"
      },
      "id": "9Z36G7BcgmDF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70748ea0",
      "metadata": {
        "id": "70748ea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad605e6-5e4e-460d-9b54-be975fc46222"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['looooot',\n",
              " 'schnazzy',\n",
              " 'timex',\n",
              " 'indiglo',\n",
              " 'jessalyn',\n",
              " 'gilsig',\n",
              " 'ruber',\n",
              " 'jaleel',\n",
              " 'balki',\n",
              " 'wavers',\n",
              " 'statistics',\n",
              " 'snapshot',\n",
              " 'guesswork',\n",
              " 'maryam',\n",
              " 'daylights',\n",
              " 'terraformed',\n",
              " 'stagnated',\n",
              " 'napolean',\n",
              " 'millimeter',\n",
              " 'enmeshed']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# 20 words that are used only once in corpus using hapaxes() function \n",
        "nltk.FreqDist(words_wo_puncts_stopwords).hapaxes()[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-4: Use the PorterStemmer to stem the words in the dataset.\n",
        "\n",
        "Display the first few words."
      ],
      "metadata": {
        "id": "US3mRSQ8bDei"
      },
      "id": "US3mRSQ8bDei"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "i = 0\n",
        "\n",
        "for word in words_wo_puncts_stopwords:\n",
        "    i+=1\n",
        "    if i == 100:\n",
        "        break\n",
        "    print(word, \" : \", ps.stem(word))"
      ],
      "metadata": {
        "id": "nX3r9FfubKdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed8fdde-94ec-4442-f5bb-0c611c6c9286"
      },
      "id": "nX3r9FfubKdB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plot  :  plot\n",
            "two  :  two\n",
            "teen  :  teen\n",
            "couples  :  coupl\n",
            "go  :  go\n",
            "church  :  church\n",
            "party  :  parti\n",
            "drink  :  drink\n",
            "drive  :  drive\n",
            "get  :  get\n",
            "accident  :  accid\n",
            "one  :  one\n",
            "guys  :  guy\n",
            "dies  :  die\n",
            "girlfriend  :  girlfriend\n",
            "continues  :  continu\n",
            "see  :  see\n",
            "life  :  life\n",
            "nightmares  :  nightmar\n",
            "deal  :  deal\n",
            "watch  :  watch\n",
            "movie  :  movi\n",
            "sorta  :  sorta\n",
            "find  :  find\n",
            "critique  :  critiqu\n",
            "mind  :  mind\n",
            "fuck  :  fuck\n",
            "movie  :  movi\n",
            "teen  :  teen\n",
            "generation  :  gener\n",
            "touches  :  touch\n",
            "cool  :  cool\n",
            "idea  :  idea\n",
            "presents  :  present\n",
            "bad  :  bad\n",
            "package  :  packag\n",
            "makes  :  make\n",
            "review  :  review\n",
            "even  :  even\n",
            "harder  :  harder\n",
            "one  :  one\n",
            "write  :  write\n",
            "since  :  sinc\n",
            "generally  :  gener\n",
            "applaud  :  applaud\n",
            "films  :  film\n",
            "attempt  :  attempt\n",
            "break  :  break\n",
            "mold  :  mold\n",
            "mess  :  mess\n",
            "head  :  head\n",
            "lost  :  lost\n",
            "highway  :  highway\n",
            "memento  :  memento\n",
            "good  :  good\n",
            "bad  :  bad\n",
            "ways  :  way\n",
            "making  :  make\n",
            "types  :  type\n",
            "films  :  film\n",
            "folks  :  folk\n",
            "snag  :  snag\n",
            "one  :  one\n",
            "correctly  :  correctli\n",
            "seem  :  seem\n",
            "taken  :  taken\n",
            "pretty  :  pretti\n",
            "neat  :  neat\n",
            "concept  :  concept\n",
            "executed  :  execut\n",
            "terribly  :  terribl\n",
            "problems  :  problem\n",
            "movie  :  movi\n",
            "well  :  well\n",
            "main  :  main\n",
            "problem  :  problem\n",
            "simply  :  simpli\n",
            "jumbled  :  jumbl\n",
            "starts  :  start\n",
            "normal  :  normal\n",
            "downshifts  :  downshift\n",
            "fantasy  :  fantasi\n",
            "world  :  world\n",
            "audience  :  audienc\n",
            "member  :  member\n",
            "idea  :  idea\n",
            "going  :  go\n",
            "dreams  :  dream\n",
            "characters  :  charact\n",
            "coming  :  come\n",
            "back  :  back\n",
            "dead  :  dead\n",
            "others  :  other\n",
            "look  :  look\n",
            "like  :  like\n",
            "dead  :  dead\n",
            "strange  :  strang\n",
            "apparitions  :  apparit\n",
            "disappearances  :  disappear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-5: Use the WordNetLemmatizer to lemmatize the words in the dataset.\n",
        "\n",
        "Display the first few words."
      ],
      "metadata": {
        "id": "QEVGhVGTbUMT"
      },
      "id": "QEVGhVGTbUMT"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        " \n",
        "wnl = WordNetLemmatizer()\n",
        " \n",
        "i = 0\n",
        "w = ['drivers','fools','mouses','horses']\n",
        "for words in w + words_wo_puncts_stopwords:\n",
        "    i+=1\n",
        "    if i<100:\n",
        "        print(words + \" ==>\" + wnl.lemmatize(words))\n",
        "    # if i>100 and i<200:"
      ],
      "metadata": {
        "id": "WZxMzKv4bMdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c79127-8d63-45fe-c4ae-3fec579b8408"
      },
      "id": "WZxMzKv4bMdl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drivers ==>driver\n",
            "fools ==>fool\n",
            "mouses ==>mouse\n",
            "horses ==>horse\n",
            "plot ==>plot\n",
            "two ==>two\n",
            "teen ==>teen\n",
            "couples ==>couple\n",
            "go ==>go\n",
            "church ==>church\n",
            "party ==>party\n",
            "drink ==>drink\n",
            "drive ==>drive\n",
            "get ==>get\n",
            "accident ==>accident\n",
            "one ==>one\n",
            "guys ==>guy\n",
            "dies ==>dy\n",
            "girlfriend ==>girlfriend\n",
            "continues ==>continues\n",
            "see ==>see\n",
            "life ==>life\n",
            "nightmares ==>nightmare\n",
            "deal ==>deal\n",
            "watch ==>watch\n",
            "movie ==>movie\n",
            "sorta ==>sorta\n",
            "find ==>find\n",
            "critique ==>critique\n",
            "mind ==>mind\n",
            "fuck ==>fuck\n",
            "movie ==>movie\n",
            "teen ==>teen\n",
            "generation ==>generation\n",
            "touches ==>touch\n",
            "cool ==>cool\n",
            "idea ==>idea\n",
            "presents ==>present\n",
            "bad ==>bad\n",
            "package ==>package\n",
            "makes ==>make\n",
            "review ==>review\n",
            "even ==>even\n",
            "harder ==>harder\n",
            "one ==>one\n",
            "write ==>write\n",
            "since ==>since\n",
            "generally ==>generally\n",
            "applaud ==>applaud\n",
            "films ==>film\n",
            "attempt ==>attempt\n",
            "break ==>break\n",
            "mold ==>mold\n",
            "mess ==>mess\n",
            "head ==>head\n",
            "lost ==>lost\n",
            "highway ==>highway\n",
            "memento ==>memento\n",
            "good ==>good\n",
            "bad ==>bad\n",
            "ways ==>way\n",
            "making ==>making\n",
            "types ==>type\n",
            "films ==>film\n",
            "folks ==>folk\n",
            "snag ==>snag\n",
            "one ==>one\n",
            "correctly ==>correctly\n",
            "seem ==>seem\n",
            "taken ==>taken\n",
            "pretty ==>pretty\n",
            "neat ==>neat\n",
            "concept ==>concept\n",
            "executed ==>executed\n",
            "terribly ==>terribly\n",
            "problems ==>problem\n",
            "movie ==>movie\n",
            "well ==>well\n",
            "main ==>main\n",
            "problem ==>problem\n",
            "simply ==>simply\n",
            "jumbled ==>jumbled\n",
            "starts ==>start\n",
            "normal ==>normal\n",
            "downshifts ==>downshift\n",
            "fantasy ==>fantasy\n",
            "world ==>world\n",
            "audience ==>audience\n",
            "member ==>member\n",
            "idea ==>idea\n",
            "going ==>going\n",
            "dreams ==>dream\n",
            "characters ==>character\n",
            "coming ==>coming\n",
            "back ==>back\n",
            "dead ==>dead\n",
            "others ==>others\n",
            "look ==>look\n",
            "like ==>like\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uMhXtA3RbMJk"
      },
      "id": "uMhXtA3RbMJk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2-6: \n",
        "a) How many unique words are there once stemming is applied? (show the that performs the computation and outputs the result)\n",
        "\n",
        "b) How many unique words are there once lemmatization is applied? (show the code that performs the computation and outputs the result)"
      ],
      "metadata": {
        "id": "LuWCQWX3bnsD"
      },
      "id": "LuWCQWX3bnsD"
    },
    {
      "cell_type": "code",
      "source": [
        "#Unique words after Stemmings\n",
        "# from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "visitedStemmers = set()\n",
        "\n",
        "for word in words_wo_puncts_stopwords:\n",
        "    # print(word, \" : \", ps.stem(word))\n",
        "    visitedStemmers.add(ps.stem(word))\n",
        "print(\"Unique words after Stemmings are = \", len(visitedStemmers))\n",
        "\n",
        "#Unique words after Lemmatization\n",
        " \n",
        "wnl = WordNetLemmatizer()\n",
        " \n",
        "visitedWordnet = set()\n",
        "for words in words_wo_puncts_stopwords:\n",
        "    visitedWordnet.add(wnl.lemmatize(words))\n",
        "    # print(words + \" ==>\" + wnl.lemmatize(words))\n",
        "print(\"Unique words after Lemmatization are = \", len(visitedWordnet))\n"
      ],
      "metadata": {
        "id": "N4WAh6UEbqNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93775c92-088c-4cc8-c5fc-14330196c8e5"
      },
      "id": "N4WAh6UEbqNq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words after Stemmings are =  26101\n",
            "Unique words after Lemmatization are =  35172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3. Tokenization "
      ],
      "metadata": {
        "id": "FQOoke3_bvbr"
      },
      "id": "FQOoke3_bvbr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3-1: Use the Penn Tree Bank tokenizer to tokenize the sentence below\n",
        "\n",
        "Print the tokens that the tokenizer produces."
      ],
      "metadata": {
        "id": "o0HA5ds8HL6-"
      },
      "id": "o0HA5ds8HL6-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204dbae3",
      "metadata": {
        "id": "204dbae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a57996a-933b-4818-91b7-ff84c9a70b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Please', 'pay', '$', '100.55', 'to', 'settle', 'your', 'bill.', 'Send', 'confirmation', 'to', 'confirm', '@', 'gmail.com', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = 'Please pay $100.55 to settle your bill.  Send confirmation to confirm@gmail.com.'\n",
        "print(TreebankWordTokenizer().tokenize(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Levenshtein Distance & Alignment\n",
        "\n",
        "Relevant nltk documentation: https://www.nltk.org/api/nltk.metrics.distance.html"
      ],
      "metadata": {
        "id": "Tu9xug2Gxr84"
      },
      "id": "Tu9xug2Gxr84"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4-1: Use the nltk functions edit_distance to compute the Levenshtein edit-distance between the strings \"intention\" and \"execution\""
      ],
      "metadata": {
        "id": "fBsXnDQ-yPPE"
      },
      "id": "fBsXnDQ-yPPE"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "edit_distance('intenstion', 'execution')"
      ],
      "metadata": {
        "id": "5aaSK4Ehylz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c81293-0e59-43da-da7a-1f180a315bbb"
      },
      "id": "5aaSK4Ehylz7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4-2: Use the nltk function edit_distance_align to compute the minimum Levenshtein edit-distance based alignment mapping between the two strings \"intention\" and \"execution\""
      ],
      "metadata": {
        "id": "NKWLhn1RzBGv"
      },
      "id": "NKWLhn1RzBGv"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance_align\n",
        "edit_distance_align('intention','execution')"
      ],
      "metadata": {
        "id": "Zc16veVuzBxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a94111a-bda4-43f3-d205-fb39b5c0e7d4"
      },
      "id": "Zc16veVuzBxM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (1, 1),\n",
              " (2, 2),\n",
              " (3, 3),\n",
              " (4, 3),\n",
              " (5, 4),\n",
              " (6, 5),\n",
              " (6, 6),\n",
              " (7, 7),\n",
              " (8, 8),\n",
              " (9, 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}